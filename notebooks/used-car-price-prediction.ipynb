{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31a3485",
   "metadata": {},
   "source": [
    "# Authenticating with Kaggle using kaggle.json\n",
    "\n",
    "Navigate to https://www.kaggle.com. \n",
    "Then go to the [Account tab of your user profile](https://www.kaggle.com/me/account) and select Create API Token. \n",
    "This will trigger the download of `kaggle.json`, a file containing your API credentials.\n",
    "\n",
    "Drag the `kaggle.json` file you downloaded on your local machine \n",
    "to the `~/mlops-zoomcamp-project` on the remote machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca78ca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ubuntu ubuntu 64 Jul 13 19:08 /home/ubuntu/mlops-zoomcamp-project/kaggle.json\r\n"
     ]
    }
   ],
   "source": [
    "# Let's make sure the kaggle.json file is present.\n",
    "!ls -lha ~/mlops-zoomcamp-project/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641c8898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy this file to ~/.kaggle\n",
    "!rm -rf ~/.kaggle/\n",
    "!mkdir -p ~/.kaggle/\n",
    "!cp ~/mlops-zoomcamp-project/kaggle.json ~/.kaggle\n",
    "\n",
    "# Change the permission to avoid a warning when starting the Kaggle tool.\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3dc7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle API 1.5.15\r\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "\n",
    "!kaggle --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb9c331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boisalai\n",
      "/home/ubuntu/mlops-zoomcamp-project/notebooks\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "\n",
    "api = kaggle.api\n",
    "print(api.get_config_value(\"username\"))\n",
    "print(api.get_default_download_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b3225c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403 - Forbidden\r\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions list\n",
    "# !kaggle datasets download -d rounakbanik/the-movies-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed97bfb",
   "metadata": {},
   "source": [
    "# Used Car Price Prediction\n",
    "\n",
    "This [data](https://www.kaggle.com/datasets/austinreese/craigslist-carstrucks-data) contains \n",
    "most all relevant information that Craigslist provides on car sales including \n",
    "columns like price, condition, manufacturer, latitude/longitude, and 18 other categories.\n",
    "\n",
    "This notebook was built from the following:\n",
    "\n",
    "* https://www.kaggle.com/code/maciejautuch/car-price-prediction\n",
    "* https://www.kaggle.com/code/hemprakashprasanna/used-car-price-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b359f7",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22259b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle URL dataset\n",
    "# https://www.kaggle.com/datasets/austinreese/craigslist-carstrucks-data\n",
    "DATASET = 'austinreese/craigslist-carstrucks-data'\n",
    "FILE_NAME = 'vehicles.csv'\n",
    "PATH = '/home/ubuntu/mlops-zoomcamp-project/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a85288e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "\n",
    "try:\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_file(DATASET, FILE_NAME, path=PATH)\n",
    "except kaggle.api.rest.ApiException as exception:\n",
    "    print(exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94056323",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ubuntu/mlops-zoomcamp-project/data/vehicles.csv.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mFILE_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.zip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops-project/lib/python3.9/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops-project/lib/python3.9/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops-project/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops-project/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops-project/lib/python3.9/site-packages/pandas/io/common.py:782\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# ZIP Compression\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compression \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_BytesZipFile\" has incompatible type\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# \"Union[str, BaseBuffer]\"; expected \"Union[Union[str, PathLike[str]],\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# ReadBuffer[bytes], WriteBuffer[bytes]]\"\u001b[39;00m\n\u001b[0;32m--> 782\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[43m_BytesZipFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompression_args\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handle\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    786\u001b[0m         handles\u001b[38;5;241m.\u001b[39mappend(handle)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops-project/lib/python3.9/site-packages/pandas/io/common.py:1025\u001b[0m, in \u001b[0;36m_BytesZipFile.__init__\u001b[0;34m(self, file, mode, archive_name, **kwargs)\u001b[0m\n\u001b[1;32m   1021\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, zipfile\u001b[38;5;241m.\u001b[39mZIP_DEFLATED)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"ZipFile\" has incompatible type \"Union[\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# Union[str, PathLike[str]], ReadBuffer[bytes], WriteBuffer[bytes]]\";\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# expected \"Union[Union[str, PathLike[str]], IO[bytes]]\"\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops-project/lib/python3.9/zipfile.py:1248\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ubuntu/mlops-zoomcamp-project/data/vehicles.csv.zip'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f'{PATH}/{FILE_NAME}.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f82e3c",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e80d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f476cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0713fc5",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of nulls in each of the above features.\n",
    "nulls_perc = df.isna().sum()/len(df)*100\n",
    "nulls_perc[nulls_perc.values>0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3debb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'county' and 'size' features which have more than 50% of their data missing\n",
    "df.drop(['county','size'], axis = 'columns', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52235c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features which have less than 5% of their data missing\n",
    "lst = nulls_perc[(nulls_perc.values>0) & (nulls_perc.values<5)].sort_values(ascending=False).index\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c765ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows in the above features that have missing values\n",
    "for features in lst:\n",
    "    df.dropna(subset=[features], inplace=True, axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Calculate the percentage of nulls in each of the above features\n",
    "nulls_perc = df.isna().sum()/len(df)*100\n",
    "nulls_perc[nulls_perc.values>0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7bcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Others columns with missing.\n",
    "def value_counts(column: str) -> None:\n",
    "    print(f\"Column name: {column}\")\n",
    "    print(df[column].value_counts(dropna=False))\n",
    "    print(\" \")\n",
    "    \n",
    "value_counts('cylinders')\n",
    "value_counts('condition')\n",
    "value_counts('VIN')\n",
    "value_counts('drive')\n",
    "value_counts('paint_color')\n",
    "value_counts('type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d69268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripping the word 'cylinders' from the 'cylinders' feature.\n",
    "df['cylinders'] = df['cylinders'].replace('cylinders','',regex=True)\n",
    "df['cylinders'] = df['cylinders'].str.strip()\n",
    "df['cylinders'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'VIN' column which is useless.\n",
    "df.drop(['VIN'], axis = 'columns', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the ratio of non-null values in the feature.\n",
    "import numpy as np\n",
    "\n",
    "def fill_missing(column: str) -> None:\n",
    "    counts = df[column].value_counts(normalize=True)\n",
    "    df[column] = df[column].fillna(\n",
    "        pd.Series(np.random.choice(\n",
    "            list(counts.index), p=list(counts.values), size=len(df)\n",
    "        ))\n",
    "    )\n",
    "    value_counts('paint_color')\n",
    "\n",
    "fill_missing('paint_color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c96532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the rows that contain missing values\n",
    "df.dropna(axis='index', inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95675140",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2432bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding the data structure and looking at car price distribution\n",
    "lower_limit = np.percentile(df[['price']], 5)\n",
    "upper_limit = np.percentile(df[['price']], 95)\n",
    "print(lower_limit, upper_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers\n",
    "df = df[(df['price'] >= lower_limit) & (df['price'] <= upper_limit)]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fbc88",
   "metadata": {},
   "source": [
    "## Create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['posting_year'] = df['posting_date'].str[0:4].astype('int64')\n",
    "df['years_used'] = df['posting_year'] - df['year']\n",
    "\n",
    "# Changing year for a smaller number.\n",
    "df['year'] = df['year'].astype('int64') - 1900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3bd137",
   "metadata": {},
   "source": [
    "## Label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec2ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical data encoding - label enncoding\n",
    "df['title_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8486f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df['title_status'] = label_encoder.fit_transform(df['title_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e4ea2",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking\n",
    "df['years_used'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674906de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a -1 value in the years_used feature. This may have happened due to some error during listing.\n",
    "df = df[df.years_used > -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca353307",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6db39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove others columns that will not be used.\n",
    "df.drop(columns=['id', 'url', 'region', 'region_url', \n",
    "                 'image_url', 'description',\n",
    "                 'lat', 'long', 'posting_date'], axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d1493f",
   "metadata": {},
   "source": [
    "# Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1813fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and label.\n",
    "x = df.drop(columns=['price','model','state']) \n",
    "y = df[['price']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical data encoding.\n",
    "x = pd.get_dummies(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ccd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test.\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e9ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"x_train {x_train.shape}\")\n",
    "print(f\"x_test {x_test.shape}\")\n",
    "print(f\"y_train {y_train.shape}\")\n",
    "print(f\"y_test {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03141424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "lm.score(x_train,y_train), lm.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5714f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train.values.ravel())\n",
    "gnb.score(x_train,y_train), gnb.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3.\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor()\n",
    "neigh.fit(x_train, y_train)\n",
    "neigh.score(x_train,y_train), neigh.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61cece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4.\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "poly.fit(x_train, x_test)\n",
    "\n",
    "x_train_poly = poly.transform(x_train)\n",
    "x_test_poly = poly.transform(x_test)\n",
    "\n",
    "lm.fit(x_train_poly, y_train)\n",
    "lm.score(x_train_poly,y_train), lm.score(x_test_poly, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd099f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5.\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor(random_state=0, max_depth=1000, \n",
    "                            min_samples_split = 18, min_impurity_decrease = 1.4)\n",
    "dtr.fit(x_train, y_train.values.ravel())\n",
    "dtr.score(x_train,y_train), dtr.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19329ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 6.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "random_forest = RandomForestRegressor(n_estimators = 250, max_features = 'sqrt', n_jobs = 20)\n",
    "random_forest.fit(x_train, y_train.values.ravel())\n",
    "print(random_forest.score(x_train, y_train), random_forest.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 7.\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagging = BaggingRegressor(n_estimators = 200, oob_score = True, n_jobs = 10)\n",
    "bagging.fit(x_train, y_train.values.ravel())\n",
    "bagging.score(x_train,y_train), bagging.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b724410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 8.\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "etr = ExtraTreesRegressor(random_state=0, n_estimators = 250, max_features = None, min_samples_split = 6)\n",
    "etr.fit(x_train, y_train.values.ravel())\n",
    "etr.score(x_train,y_train), etr.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e89c48c",
   "metadata": {},
   "source": [
    "# Choosing the next best algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc8449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def create_models():\n",
    "    models = []\n",
    "    models.append(('Linear Regression', LinearRegression()))\n",
    "    models.append(('Decision Tree Regressor', DecisionTreeRegressor()))\n",
    "    models.append(('ElasticNet_Regressor', ElasticNet()))\n",
    "    models.append(('Lasso_Regressor', Lasso()))\n",
    "    models.append(('Ridge_Regressor', Ridge()))\n",
    "    models.append(('RandomForest_Regressor', RandomForestRegressor()))\n",
    "    return models\n",
    "\n",
    "# creating a list with all the algorithms we are going to assess\n",
    "models = create_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74815cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "for name, model in models:\n",
    "    print(\" \")\n",
    "    print(name)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    print('Train R2 :', r2_score(y_train, y_pred_train))\n",
    "    print('Test R2 :', r2_score(y_test, y_pred_test))\n",
    "    print('Train RMSE :', np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "    print('Test RMSE :', np.sqrt(mean_squared_error(y_test, y_pred_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810f226",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning for Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f4a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "grid_parameters = {'n_estimators': [80, 90, 100, 110],'max_depth': [5, 6],\n",
    "                   'max_features': [None, 'auto'], 'min_samples_split': [2, 3]}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=1),\n",
    "    param_distributions=grid_parameters,\n",
    "    cv=5,n_iter=10,n_jobs=-1)\n",
    "\n",
    "random_search.fit(x_train, y_train)\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae831540",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=random_search.best_params_.get('n_estimators'),\n",
    "                            max_depth=random_search.best_params_.get('max_depth'),\n",
    "                            min_samples_split=random_search.best_params_.get('min_samples_split'),\n",
    "                            max_features=random_search.best_params_.get('max_features'),\n",
    "                            random_state=1)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "print('Train R2 :', r2_score(y_train, y_pred_train))\n",
    "print('Test R2 :', r2_score(y_test, y_pred_test))\n",
    "print('Train RMSE :', np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "print('Test RMSE :', np.sqrt(mean_squared_error(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b2c4e",
   "metadata": {},
   "source": [
    "# XGBoost Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor(random_state=1).fit(x_train,y_train)\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "print('Train R2 :', r2_score(y_train, y_pred_train))\n",
    "print('Test R2 :', r2_score(y_test, y_pred_test))\n",
    "print('Train RMSE :', np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "print('Test RMSE :', np.sqrt(mean_squared_error(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223c2c5",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for XGBoost Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_params = {'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6],\n",
    "                 'max_depth':range(3,10),\n",
    "                 'gamma':[0,1,2,3,4]}\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(),\n",
    "    param_distributions=tuning_params,\n",
    "    cv=5,n_iter=10,n_jobs=1)\n",
    "\n",
    "xgb_search.fit(x_train, y_train)\n",
    "print(xgb_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(learning_rate=0.6,max_depth=9,gamma=4).fit(x_train, y_train)\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "print('Train R2 :', r2_score(y_train, y_pred_train))\n",
    "print('Test R2 :', r2_score(y_test, y_pred_test))\n",
    "print('Train RMSE :', np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "print('Test RMSE :', np.sqrt(mean_squared_error(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3691d4cc",
   "metadata": {},
   "source": [
    "# Save Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c17d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(f'{PATH}/{FILE_NAME}', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
